{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManuscriptS5a - Predicting task performance using information estimates (prior to information transfer mapping)\n",
    "\n",
    "## Analysis statistic in text, but contributes to analysis in Supplementary Figure 5\n",
    "\n",
    "\n",
    "## Master code for Ito et al., 2017Â¶\n",
    "#### Takuya Ito (takuya.ito@rutgers.edu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import numpy as np\n",
    "import loadGlasser as lg\n",
    "import scripts3_functions as func\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import multregressionconnectivity as mreg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "# Set basic parameters\n",
    "basedir = '/projects2/ModalityControl2/'\n",
    "datadir = basedir + 'data/'\n",
    "resultsdir = datadir + 'resultsMaster/'\n",
    "runLength = 4648\n",
    "\n",
    "subjNums = ['032', '033', '037', '038', '039', '045', \n",
    "            '013', '014', '016', '017', '018', '021', \n",
    "            '023', '024', '025', '026', '027', '031', \n",
    "            '035', '046', '042', '028', '048', '053', \n",
    "            '040', '049', '057', '062', '050', '030', '047', '034']\n",
    "\n",
    "glasserparcels = lg.loadGlasserParcels()\n",
    "networkdef = lg.loadGlasserNetworks()\n",
    "# Need to flip\n",
    "tmp = np.zeros((networkdef.shape[0]))\n",
    "tmp[0:180] = networkdef[180:]\n",
    "tmp[180:] = networkdef[0:180]\n",
    "networkdef = tmp\n",
    "\n",
    "networkmappings = {'fpn':7, 'vis':1, 'smn':2, 'con':3, 'dmn':6, 'aud1':8, 'aud2':9, 'dan':11}\n",
    "# Force aud2 key to be the same as aud1\n",
    "aud2_ind = np.where(networkdef==networkmappings['aud2'])[0]\n",
    "networkdef[aud2_ind] = networkmappings['aud1']\n",
    "# Define new network mappings with no aud1/aud2 distinction\n",
    "networkmappings = {'fpn':7, 'vis':1, 'smn':2, 'con':3, 'dmn':6, 'aud':8, 'dan':11,\n",
    "                   'prem':5, 'pcc':10, 'none':12, 'hipp':13, 'pmulti':14}\n",
    "\n",
    "nParcels = 360\n",
    "\n",
    "# Import network reordering\n",
    "networkdir = '/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1/'\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "order = networkorder\n",
    "order.shape = (len(networkorder),1)\n",
    "\n",
    "# Construct xticklabels and xticks for plotting figures\n",
    "networks = networkmappings.keys()\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[order]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "    \n",
    "# Load in Glasser parcels in their native format\n",
    "glasser2 = nib.load('/projects/AnalysisTools/ParcelsGlasser2016/archive/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii')\n",
    "glasser2 = glasser2.get_data()\n",
    "glasser2 = glasser2[0][0][0][0][0]\n",
    "\n",
    "accdata = {}\n",
    "rtdata = {}\n",
    "for subj in subjNums:\n",
    "    filename = '/projects2/ModalityControl2/data/resultsMaster/behavresults/' + subj+'_acc_by_mb.txt'\n",
    "    accdata[subj] = np.loadtxt(filename)\n",
    "    filename = '/projects2/ModalityControl2/data/resultsMaster/behavresults/' + subj+'_rt_by_mb.txt'\n",
    "    rtdata[subj] = np.loadtxt(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load in preprocessed vertex-wise betas across all miniblocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define some basic functions for RSA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadBetas(subj):\n",
    "    datadir = '/projects2/ModalityControl2/data/resultsMaster/glmMiniblockBetaSeries/'\n",
    "    filename = subj + '_miniblock_taskBetas_Surface64k.csv'\n",
    "    betas = np.loadtxt(datadir + filename, delimiter=',')\n",
    "    betas = betas[:,17:].T\n",
    "    return betas\n",
    "\n",
    "\n",
    "def setUpRSAMatrix(subj,ruledim):\n",
    "    \"\"\"\n",
    "    Sets up basic SVM Matrix for a classification of a particular rule dimension and network\n",
    "    \"\"\"\n",
    "    \n",
    "    betas = loadBetas(subj)\n",
    "    rules, rulesmb = func.importRuleTimingsV3(subj,ruledim)\n",
    "    \n",
    "    svm_mat = np.zeros((betas.shape))\n",
    "    samplecount = 0\n",
    "    labels = []\n",
    "    for rule in rulesmb:\n",
    "        rule_ind = rulesmb[rule].keys()\n",
    "        sampleend = samplecount + len(rule_ind)\n",
    "        svm_mat[samplecount:sampleend,:] = betas[rule_ind,:]\n",
    "        labels.extend(np.ones(len(rule_ind),)*rule)\n",
    "        samplecount += len(rule_ind)\n",
    "        \n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    svm_dict = {}\n",
    "    nParcels = 360\n",
    "    for roi in range(1,nParcels+1):\n",
    "        roi_ind = np.where(glasserparcels==roi)[0]\n",
    "        svm_dict[roi] = svm_mat[:,roi_ind]\n",
    "    \n",
    "    return svm_dict, labels\n",
    "\n",
    "def rsaCV(svm_mat,labels, subj):\n",
    "    \"\"\"Runs a leave-4-out CV for a 4 way  classification\"\"\"\n",
    "        \n",
    "    cvfolds = []\n",
    "    # 32 folds, if we do a leave 4 out for 128 total miniblocks\n",
    "    # Want to leave a single block from each rule from each CV\n",
    "    for rule in np.unique(labels):\n",
    "        cvfolds.append(np.where(labels==rule)[0])\n",
    "    cvfolds = np.asarray(cvfolds)\n",
    "    \n",
    "    # Number of CVs is columns\n",
    "    ncvs = cvfolds.shape[1]\n",
    "    nrules = cvfolds.shape[0]\n",
    "    # Randomly sample cross-validation folds\n",
    "    for i in range(nrules): np.random.shuffle(cvfolds[i,:])\n",
    "    \n",
    "    corr_rho_cvs = []\n",
    "    err_rho_cvs = []\n",
    "    acc_ind = []\n",
    "    infoEstimate = np.zeros((labels.shape[0],))\n",
    "    for cv in range(ncvs):\n",
    "        # Select a test set from the CV Fold matrix\n",
    "        test_ind = cvfolds[:,cv].copy()\n",
    "        # Delete the CV included from the train set\n",
    "        train_ind = np.delete(cvfolds,cv,axis=1)\n",
    "        \n",
    "        # Identify the train and test sets\n",
    "        svm_train = svm_mat[np.reshape(train_ind,-1),:]\n",
    "        svm_test = svm_mat[test_ind,:]\n",
    "\n",
    "        \n",
    "        prototype = {}\n",
    "        # Construct RSA prototypes\n",
    "        for rule in range(nrules):\n",
    "            prototype_ind = np.reshape(train_ind[rule,:],-1)\n",
    "            prototype[rule] = np.mean(svm_mat[prototype_ind],axis=0)\n",
    "            \n",
    "        corr_rho = []\n",
    "        err_rho = []\n",
    "        for rule1 in range(nrules):\n",
    "            tmp = []\n",
    "            for rule2 in range(nrules):\n",
    "                r = stats.spearmanr(prototype[rule1],svm_test[rule2])[0]\n",
    "                r = np.arctanh(r)\n",
    "                if rule1==rule2: \n",
    "                    corr_rho.append(r)\n",
    "                else:\n",
    "                    tmp.append(r)\n",
    "            err_rho.append(np.mean(tmp))\n",
    "\n",
    "        corr_rho_cvs.append(np.mean(corr_rho))\n",
    "        err_rho_cvs.append(np.mean(err_rho))\n",
    "        # Compute miniblock-wise information estimate\n",
    "        for i in range(len(corr_rho)):\n",
    "            # Re-assign information estimate to original miniblock\n",
    "            infoEstimate[cvfolds[i,cv]] = (corr_rho[i] - err_rho[i])\n",
    "\n",
    "    return infoEstimate\n",
    "        \n",
    "    \n",
    "def subjRSACV((subj,behav)):\n",
    "    ruledims = ['logic','sensory','motor']\n",
    "    svm_dict = {}\n",
    "    labels = {}\n",
    "    for ruledim in ruledims:\n",
    "        svm_dict[ruledim], labels[ruledim] = setUpRSAMatrix(subj,ruledim)\n",
    "\n",
    "    logit_beta = np.zeros((nParcels,))\n",
    "    predictions = np.zeros((nParcels,128))\n",
    "    accuracy = np.zeros((128,))\n",
    "    roicount = 0\n",
    "    for roi in svm_dict['logic']:\n",
    "        infoEstimate = np.zeros((128,3)) # nMiniblocks, nRuledims\n",
    "        rulecount = 0\n",
    "        for ruledim in ruledims:\n",
    "            svm_mat = svm_dict[ruledim][roi].copy()\n",
    "            # Demean each sample\n",
    "            svmmean = np.mean(svm_mat,axis=1)\n",
    "            svmmean.shape = (len(svmmean),1)\n",
    "            svm_mat = svm_mat - svmmean\n",
    "\n",
    "\n",
    "            infoEstimate[:,rulecount] = rsaCV(svm_mat, \n",
    "                                              labels[ruledim], \n",
    "                                              subj)\n",
    "            \n",
    "            # Normalize information estimates\n",
    "#             infoEstimate = stats.zscore(infoEstimate,axis=0)\n",
    "            \n",
    "            rulecount += 1\n",
    "        \n",
    "        # Sum across all rule dimension\n",
    "#         infoEstimate = np.sum(infoEstimate,axis=1)\n",
    "#         infoEstimate = np.mean(infoEstimate,axis=1)\n",
    "        \n",
    "        # Run logistic (or correlation) with behavior (performance or RT)\n",
    "#         ind_var = np.vstack((np.ones((len(infoEstimate),)),infoEstimate.T))\n",
    "#         ind_var = ind_var.T\n",
    "        \n",
    "        if behav=='acc':\n",
    "            accuracy = accdata[subj]\n",
    "            # binarize accuracy\n",
    "#             accuracy = accuracy>.5\n",
    "            # Run cross-validation\n",
    "            prediction = np.zeros((len(accuracy),))\n",
    "            for mb in range(len(accuracy)):\n",
    "                indices = np.arange(len(accuracy))\n",
    "                testset_ind = mb\n",
    "                trainset_ind = np.delete(indices,mb)\n",
    "                \n",
    "                # Define train and test set\n",
    "                trainset = infoEstimate[trainset_ind,:]\n",
    "                testset = infoEstimate[testset_ind,:]\n",
    "\n",
    "                # Z-normalize train set and normalize test set using trainset's mean and std\n",
    "#                 mean_train = np.mean(trainset,axis=0)\n",
    "#                 mean_train.shape = (1,trainset.shape[1])\n",
    "#                 std_train = np.std(trainset,axis=0)\n",
    "#                 std_train.shape = (1,trainset.shape[1])\n",
    "                \n",
    "#                 # Normalize train set\n",
    "#                 trainset = np.divide((trainset - mean_train),std_train)\n",
    "#                 # Normalize test set\n",
    "#                 testset = np.divide((testset - mean_train),std_train)\n",
    "#                 testset = testset.T\n",
    "                \n",
    "                # Create regressor matrix and pad trainset with ones\n",
    "                regressors = np.vstack((np.ones((trainset.shape[0],)),trainset.T))\n",
    "                regressors = regressors.T\n",
    "                logit = sm.Logit(accuracy[trainset_ind],regressors)\n",
    "                result = logit.fit(disp=False)\n",
    "                b = result.params\n",
    "                \n",
    "                # Predict model output\n",
    "                # Don't include the bias term since we want to remove bias towards correct predictions\n",
    "                y = 1/(1+np.exp(-(b[1]*testset[0] + b[2]*testset[1] + b[3]*testset[2])))\n",
    "                \n",
    "                prediction[testset_ind] = y\n",
    "        \n",
    "            # Classify\n",
    "            binary_predictions = prediction > .5\n",
    "            binary_actual = accuracy > .5\n",
    "            classification = np.mean(binary_actual==binary_predictions)\n",
    "            betas = classification\n",
    "#             betas = stats.spearmanr(accuracy,prediction)[0]\n",
    "#             predictions[roicount,:] = prediction\n",
    "            # Fit model\n",
    "#             betas = result.params[1:] # Get only infoestimate beta params\n",
    "\n",
    "        elif behav=='rt':\n",
    "            accuracy = rtdata[subj]\n",
    "            betas = stats.spearmanr(infoEstimate,accuracy)[0]\n",
    "        \n",
    "        logit_beta[roicount] = betas\n",
    "        \n",
    "        roicount += 1\n",
    "    \n",
    "    return logit_beta \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Run logistic regressions comparing accuracy to information estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "behav='acc'\n",
    "\n",
    "inputs = []\n",
    "for subj in subjNums: inputs.append((subj,behav))\n",
    "\n",
    "pool = mp.Pool(processes=32)\n",
    "results = pool.map_async(subjRSACV,inputs).get()\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "logit_beta = np.zeros((nParcels,len(subjNums)))\n",
    "scount = 0\n",
    "for result in results:\n",
    "    for roi in range(nParcels):\n",
    "        logit_beta[roi,scount] = result[roi]\n",
    "        \n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now visualize correlation with behavior (logistic regression results) for only regions that have significant information estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_stats = {}\n",
    "# Output to CSV matrix\n",
    "sig_t = np.zeros((nParcels,))\n",
    "sig_effect = np.zeros((nParcels,))\n",
    "effectsize = {}\n",
    "logit_stats = {}\n",
    "logit_stats['t'] = np.zeros((nParcels,))\n",
    "logit_stats['p'] = np.zeros((nParcels,))\n",
    "logit_stats['q'] = np.zeros((nParcels,))\n",
    "effectsize = []\n",
    "for roi in range(nParcels):\n",
    "    t, p = stats.ttest_1samp(logit_beta[roi,:],.5)\n",
    "\n",
    "    effectsize.append(np.mean(logit_beta[roi,:]))\n",
    "\n",
    "    if t > 0:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0 - p/2.0\n",
    "    logit_stats['t'][roi] = t\n",
    "    logit_stats['p'][roi] = p\n",
    "\n",
    "arr = logit_stats['p']\n",
    "# Include all ROIs\n",
    "sig_only = np.where(logit_stats['p']<1)[0]\n",
    "\n",
    "### \n",
    "# Only run statistics on FPN & CON regions\n",
    "sig_only = np.where((networkdef==networkmappings['fpn']) | (networkdef==networkmappings['con']))[0]\n",
    "\n",
    "## Get Network definitions and info\n",
    "# sig_only = np.arange(nParcels)\n",
    "# sig_only = np.where(networkdef==networkmappings['smn'])[0]\n",
    "# sig_only = np.where(networkdef==networkmappings['con'])[0]\n",
    "#     sig_only = np.intersect1d(sig_only,sig_only2)\n",
    "#     sig_only = np.where\n",
    "qmat = np.ones((len(arr),))\n",
    "arr = np.asarray(arr)\n",
    "qmat[sig_only] = mc.fdrcorrection0(arr[sig_only])[1]\n",
    "for roi in range(nParcels):\n",
    "    logit_stats['q'][roi] = qmat[roi]\n",
    "\n",
    "tarr = np.asarray(logit_stats['t'])\n",
    "qarr = np.asarray(logit_stats['q'])\n",
    "\n",
    "qbin = qarr < 0.05\n",
    "sig_t[:] = np.multiply(tarr,qbin)\n",
    "sig_effect[:] = np.multiply(np.asarray(effectsize),qbin)\n",
    "##\n",
    "# added this after finding that transfers between region 260 and 271 were significant\n",
    "sig_effect[271] = .53002875 # Decoding accuracy of ITE from 260 -> 271\n",
    "\n",
    "sig_t_vertex = np.zeros((len(glasser2),))\n",
    "effects_vertex = np.zeros((len(glasser2),2))\n",
    "col = 0\n",
    "for roi in range(nParcels):\n",
    "    parcel_ind = np.where(glasser2==(roi+1))[0]\n",
    "    sig_t_vertex[parcel_ind] = sig_t[roi]\n",
    "    effects_vertex[parcel_ind,0] = effectsize[roi]\n",
    "    effects_vertex[parcel_ind,1] = sig_effect[roi]\n",
    "\n",
    "\n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/ManuscriptS5a_BaselineIE_PerformanceDecoding/'\n",
    "filename = 'BaselineIE_64k_RuleGeneral_LogitRegCorrWithAcc_Tstat_FDR.csv'\n",
    "np.savetxt(outdir + filename, sig_t_vertex,fmt='%s')\n",
    "wb_file = 'BaselineIE_64k_RuleGeneral_LogitRegCorrWithAcc_Tstat_FDR.dscalar.nii'\n",
    "glasserfilename = '/projects/AnalysisTools/ParcelsGlasser2016/archive/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "# Compute effect size baseline (information content)\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/ManuscriptS5a_BaselineIE_PerformanceDecoding/'\n",
    "filename = 'BaselineIE_64k_RuleGeneral_LogitRegCorrWithAcc_EffectSize_FDR.csv'\n",
    "np.savetxt(outdir + filename, effects_vertex,fmt='%s')\n",
    "wb_file = 'BaselineIE_64k_RuleGeneral_LogitRegCorrWithAcc_EffectSize_FDR.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ROIs that are related to behavior for each rule dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values: [ 18  39  40  45  67  68  69  83  85  86  88  90  91  94  96  97 107 114\n",
      " 122 129 148 149 165 176 178 182 190 195 205 207 209 219 220 224 226 230\n",
      " 231 240 243 247 248 252 254 260 261 278 288 302 309 333 334 346 355 357]\n",
      "Significant ROIs: Searching only FPN and CON regions \n",
      "\n",
      "ROI: 260\n",
      "Decoding accuracy: 0.5263671875\n",
      "P = 0.000196346966794 | q = 0.0188493088123\n"
     ]
    }
   ],
   "source": [
    "tmp = np.where(logit_stats['p']<0.05)[0]\n",
    "networkdef[tmp]\n",
    "print 'P-values:', tmp\n",
    "# Find significant ROIs\n",
    "behav_rois = np.where(logit_stats['q']<0.05)[0]\n",
    "print 'Significant ROIs: Searching only FPN and CON regions \\n'\n",
    "for roi in behav_rois:\n",
    "    print 'ROI:', roi\n",
    "    print 'Decoding accuracy:', effectsize[roi]\n",
    "    print 'P =', logit_stats['p'][roi], '| q =', logit_stats['q'][roi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run FWE correction (using Permutation tests) for statistical significance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant ROIs: Searching only FPN and CON regions \n",
      "\n",
      "ROI: 260\n",
      "Decoding accuracy: 0.5263671875\n",
      "T-statistic: 3.97397192116\n",
      "pfwe = 0.016\n"
     ]
    }
   ],
   "source": [
    "import permutationTesting as pt\n",
    "sig_only = np.where((networkdef==networkmappings['fpn']) | (networkdef==networkmappings['con']))[0]\n",
    "chance = .5\n",
    "tmp = logit_beta[sig_only] - chance # Divide decoding accuracies by chance\n",
    "t, p = pt.permutationFWE(tmp,permutations=1000,nproc=15)\n",
    "pfwe = np.zeros((nParcels,))\n",
    "tfwe = np.zeros((nParcels,))\n",
    "pfwe[sig_only] = p\n",
    "tfwe[sig_only] = t\n",
    "\n",
    "behav_rois = np.where(pfwe>0.95)[0]\n",
    "print 'Significant ROIs: Searching only FPN and CON regions \\n'\n",
    "for roi in behav_rois:\n",
    "    print 'ROI:', roi\n",
    "    print 'Decoding accuracy:', effectsize[roi]\n",
    "    print 'T-statistic:', tfwe[roi]\n",
    "    print 'pfwe =', 1.0 - pfwe[roi]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
