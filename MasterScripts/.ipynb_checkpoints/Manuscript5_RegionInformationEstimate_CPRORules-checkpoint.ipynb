{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuscript5 - Calculating information estimate for each brain-region using vertex-level activation patterns for each rule domain\n",
    "\n",
    "## Analysis for Fig. 5\n",
    "\n",
    "\n",
    "## Master code for Ito et al., 2017Â¶\n",
    "#### Takuya Ito (takuya.ito@rutgers.edu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import numpy as np\n",
    "import loadGlasser as lg\n",
    "import scripts3_functions as func\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import multregressionconnectivity as mreg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import permutationTesting as pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "# Set basic parameters\n",
    "basedir = '/projects2/ModalityControl2/'\n",
    "datadir = basedir + 'data/'\n",
    "resultsdir = datadir + 'resultsMaster/'\n",
    "runLength = 4648\n",
    "\n",
    "subjNums = ['032', '033', '037', '038', '039', '045', \n",
    "            '013', '014', '016', '017', '018', '021', \n",
    "            '023', '024', '025', '026', '027', '031', \n",
    "            '035', '046', '042', '028', '048', '053', \n",
    "            '040', '049', '057', '062', '050', '030', '047', '034']\n",
    "\n",
    "# Organized as a 64k vector\n",
    "glasserparcels = lg.loadGlasserParcels()\n",
    "\n",
    "nParcels = 360\n",
    "\n",
    "# Load in Glasser parcels in their native format\n",
    "# Note that this parcel file is actually flipped (across hemispheres), but it doesn't matter since we're using the same exact file to reconstruct the data\n",
    "glasser2 = nib.load('/projects/AnalysisTools/ParcelsGlasser2016/archive/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii')\n",
    "glasser2 = glasser2.get_data()\n",
    "glasser2 = np.squeeze(glasser2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load in vertex-wise betas across all miniblocks for all brain regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define some basic functions for RSA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadBetas(subj):\n",
    "    datadir = '/projects2/ModalityControl2/data/resultsMaster/glmMiniblockBetaSeries/'\n",
    "    filename = subj + '_miniblock_taskBetas_Surface64k.csv'\n",
    "    betas = np.loadtxt(datadir + filename, delimiter=',')\n",
    "    betas = betas[:,17:].T\n",
    "    return betas\n",
    "\n",
    "\n",
    "def setUpRSAMatrix(subj,ruledim):\n",
    "    \"\"\"\n",
    "    Sets up basic SVM Matrix for a classification of a particular rule dimension and network\n",
    "    \"\"\"\n",
    "    \n",
    "    betas = loadBetas(subj)\n",
    "    rules, rulesmb = func.importRuleTimingsV3(subj,ruledim)\n",
    "    \n",
    "    svm_mat = np.zeros((betas.shape))\n",
    "    samplecount = 0\n",
    "    labels = []\n",
    "    for rule in rulesmb:\n",
    "        rule_ind = rulesmb[rule].keys()\n",
    "        sampleend = samplecount + len(rule_ind)\n",
    "        svm_mat[samplecount:sampleend,:] = betas[rule_ind,:]\n",
    "        labels.extend(np.ones(len(rule_ind),)*rule)\n",
    "        samplecount += len(rule_ind)\n",
    "        \n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    svm_dict = {}\n",
    "    nParcels = 360\n",
    "    for roi in range(1,nParcels+1):\n",
    "        roi_ind = np.where(glasserparcels==roi)[0]\n",
    "        svm_dict[roi] = svm_mat[:,roi_ind]\n",
    "    \n",
    "    return svm_dict, labels\n",
    "\n",
    "def rsaCV(svm_mat,labels, subj):\n",
    "    \"\"\"Runs a leave-4-out CV for a 4 way  classification\"\"\"\n",
    "        \n",
    "    cvfolds = []\n",
    "    # 32 folds, if we do a leave 4 out for 128 total miniblocks\n",
    "    # Want to leave a single block from each rule from each CV\n",
    "    for rule in np.unique(labels):\n",
    "        cvfolds.append(np.where(labels==rule)[0])\n",
    "    cvfolds = np.asarray(cvfolds)\n",
    "    \n",
    "    # Number of CVs is columns\n",
    "    ncvs = cvfolds.shape[1]\n",
    "    nrules = cvfolds.shape[0]\n",
    "    corr_rho_cvs = []\n",
    "    err_rho_cvs = []\n",
    "    acc_ind = []\n",
    "    infoEstimate = []\n",
    "    for cv in range(ncvs):\n",
    "        # Select a test set from the CV Fold matrix\n",
    "        test_ind = cvfolds[:,cv].copy()\n",
    "        # The accuracy array should be the same as test_idn\n",
    "        acc_ind.extend(cvfolds[:,cv].copy())\n",
    "        # Delete the CV included from the train set\n",
    "        train_ind = np.delete(cvfolds,cv,axis=1)\n",
    "        \n",
    "        # Identify the train and test sets\n",
    "        svm_train = svm_mat[np.reshape(train_ind,-1),:]\n",
    "        svm_test = svm_mat[test_ind,:]\n",
    "        \n",
    "        prototype = {}\n",
    "        # Construct RSA prototypes\n",
    "        for rule in range(nrules):\n",
    "            prototype_ind = np.reshape(train_ind[rule,:],-1)\n",
    "            prototype[rule] = np.mean(svm_mat[prototype_ind],axis=0)\n",
    "            \n",
    "        corr_rho = []\n",
    "        err_rho = []\n",
    "        for rule1 in range(nrules):\n",
    "            tmp = []\n",
    "            for rule2 in range(nrules):\n",
    "                r = np.arctanh(stats.spearmanr(prototype[rule1],svm_test[rule2,:])[0])\n",
    "                if rule1==rule2: \n",
    "                    corr_rho.append(r)\n",
    "                else:\n",
    "                    tmp.append(r)\n",
    "            err_rho.append(np.mean(tmp))\n",
    "\n",
    "        corr_rho_cvs.append(np.mean(corr_rho))\n",
    "        err_rho_cvs.append(np.mean(err_rho))\n",
    "        # Compute miniblock-wise information estimate\n",
    "        for i in range(len(corr_rho)):\n",
    "            infoEstimate.append(corr_rho[i] - err_rho[i])\n",
    "\n",
    "    # independent var (constant terms + information estimate)\n",
    "    infoEstimate = np.asarray(infoEstimate)\n",
    "    ind_var = np.vstack((np.ones((len(infoEstimate),)),infoEstimate))\n",
    "    ind_var = ind_var.T\n",
    "\n",
    "\n",
    "    return np.mean(corr_rho_cvs), np.mean(err_rho_cvs), np.mean(infoEstimate)\n",
    "        \n",
    "    \n",
    "def subjRSACV((subj,ruledim,behav)):\n",
    "    svm_dict, labels = setUpRSAMatrix(subj,ruledim)\n",
    "    corr_rhos = {}\n",
    "    err_rhos = {}\n",
    "    infoEstimate = {}\n",
    "    for roi in svm_dict:\n",
    "        svm_mat = svm_dict[roi].copy()\n",
    "        # Demean each sample\n",
    "        svmmean = np.mean(svm_mat,axis=1)\n",
    "        svmmean.shape = (len(svmmean),1)\n",
    "        svm_mat = svm_mat - svmmean\n",
    "\n",
    "#         svm_mat = preprocessing.scale(svm_mat,axis=0)\n",
    "\n",
    "        corr_rhos[roi], err_rhos[roi], infoEstimate[roi] = rsaCV(svm_mat, labels, subj)\n",
    "        \n",
    "    return corr_rhos, err_rhos, infoEstimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Estimate information estimates for all regions for all 3 rule domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logic\n",
      "Running sensory\n",
      "Running motor\n"
     ]
    }
   ],
   "source": [
    "ruledims = ['logic','sensory','motor']\n",
    "behav='acc'\n",
    "corr_rhos = {}\n",
    "err_rhos = {}\n",
    "diff_rhos = {}\n",
    "for ruledim in ruledims:\n",
    "    corr_rhos[ruledim] = {}\n",
    "    err_rhos[ruledim] = {}\n",
    "    diff_rhos[ruledim] = {}\n",
    "    \n",
    "    print 'Running', ruledim\n",
    "\n",
    "    inputs = []\n",
    "    for subj in subjNums: inputs.append((subj,ruledim,behav))\n",
    "\n",
    "#     pool = mp.Pool(processes=8)\n",
    "    pool = mp.Pool(processes=16)\n",
    "    results = pool.map_async(subjRSACV,inputs).get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Reorganize results\n",
    "    corr_rhos[ruledim] = np.zeros((nParcels,len(subjNums)))\n",
    "    err_rhos[ruledim] = np.zeros((nParcels,len(subjNums)))\n",
    "    diff_rhos[ruledim] = np.zeros((nParcels,len(subjNums)))\n",
    "    \n",
    "    scount = 0\n",
    "    for result in results:\n",
    "        for roi in range(nParcels):\n",
    "            corr_rhos[ruledim][roi,scount] = result[0][roi+1]\n",
    "            err_rhos[ruledim][roi,scount] = result[1][roi+1]\n",
    "            diff_rhos[ruledim][roi,scount] = result[2][roi+1]\n",
    "        scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CSVs for baseline leave-4-out CV on information transfer estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "\n",
    "for ruledim in ruledims:\n",
    "    filename = 'Regionwise_InformationEstimate_LeaveOneOut_' + ruledim + '.csv'\n",
    "    np.savetxt(outdir + filename, diff_rhos[ruledim], delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run statistics (t-tests and multiple comparisons correction with FDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stats = {}\n",
    "# Output to CSV matrix\n",
    "sig_t = np.zeros((nParcels,len(ruledims)))\n",
    "sig_effect = np.zeros((nParcels,len(ruledims)))\n",
    "effectsize = {}\n",
    "rulecount = 0\n",
    "for ruledim in ruledims:\n",
    "    df_stats[ruledim] = {}\n",
    "    df_stats[ruledim]['t'] = np.zeros((nParcels,))\n",
    "    df_stats[ruledim]['p'] = np.zeros((nParcels,))\n",
    "    effectsize[ruledim] = np.zeros((nParcels,))\n",
    "    for roi in range(nParcels):\n",
    "        t, p = stats.ttest_1samp(diff_rhos[ruledim][roi,:], 0)\n",
    "#         t, p = stats.ttest_rel(corr_rhos[ruledim][roi,:], err_rhos[ruledim][roi,:])\n",
    "            \n",
    "        effectsize[ruledim][roi] = np.mean(diff_rhos[ruledim][roi,:])\n",
    "\n",
    "        ps = np.zeros(())\n",
    "        if t > 0:\n",
    "            p = p/2.0\n",
    "        else:\n",
    "            p = 1.0 - p/2.0\n",
    "        df_stats[ruledim]['t'][roi] = t\n",
    "        df_stats[ruledim]['p'][roi] = p\n",
    "        \n",
    "    arr = df_stats[ruledim]['p']\n",
    "    df_stats[ruledim]['q'] = mc.fdrcorrection0(arr)[1]\n",
    "\n",
    "        \n",
    "\n",
    "    qbin = df_stats[ruledim]['q'] < 0.05\n",
    "    sig_t[:,rulecount] = np.multiply(df_stats[ruledim]['t'],qbin)\n",
    "    sig_effect[:,rulecount] = np.multiply(effectsize[ruledim],qbin)\n",
    "    \n",
    "    rulecount += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map statistics and results to surface using workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_t_vertex = np.zeros((len(glasser2),len(ruledims)))\n",
    "effects_vertex = np.zeros((len(glasser2),len(ruledims)))\n",
    "effects_vertex_sig = np.zeros((len(glasser2),len(ruledims)))\n",
    "col = 0\n",
    "for cols in range(sig_t_vertex.shape[1]):\n",
    "    for roi in range(nParcels):\n",
    "        parcel_ind = np.where(glasser2==(roi+1))[0]\n",
    "        sig_t_vertex[parcel_ind,col] = sig_t[roi,col]\n",
    "        effects_vertex[parcel_ind,col] = effectsize[ruledims[col]][roi]\n",
    "        effects_vertex_sig[parcel_ind,col] = sig_effect[roi,col]\n",
    "    col += 1\n",
    "\n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'RegionwiseIE_FDRThresholded_Tstat.csv'\n",
    "np.savetxt(outdir + filename, sig_t_vertex,fmt='%s')\n",
    "wb_file = 'RegionwiseIE_FDRThresholded_Tstat.dscalar.nii'\n",
    "glasserfilename = '/projects/AnalysisTools/ParcelsGlasser2016/archive/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "# Compute effect size baseline (information content)\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'RegionwiseIE_InformationEstimate.csv'\n",
    "np.savetxt(outdir + filename, effects_vertex,fmt='%s')\n",
    "wb_file = 'RegionwiseIE_InformationEstimate.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "# Compute Thresholded effect size baseline (information content)\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'RegionwiseIE_FDRThresholded_InformationEstimate.csv'\n",
    "np.savetxt(outdir + filename, effects_vertex_sig,fmt='%s')\n",
    "wb_file = 'RegionwiseIE_FDRThresholded_InformationEstimate.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run FWE Correction using permutation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt = reload(pt)\n",
    "\n",
    "fwe_Ts = np.zeros((nParcels,len(ruledims)))\n",
    "fwe_Ps = np.zeros((nParcels,len(ruledims)))\n",
    "rulecount = 0\n",
    "for ruledim in ruledims:\n",
    "    t, p = pt.permutationFWE(diff_rhos[ruledim], nullmean=0, permutations=10000, nproc=15)\n",
    "#     t, p = pt.permutationFWE(corr_rhos[ruledim] - err_rhos[ruledim],\n",
    "#                              nullmean=0, permutations=1000, nproc=15)\n",
    "    fwe_Ts[:,rulecount] = t\n",
    "    fwe_Ps[:,rulecount] = p\n",
    "    \n",
    "    rulecount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct t-values match up\n",
      "Correct t-values match up\n",
      "Correct t-values match up\n"
     ]
    }
   ],
   "source": [
    "# Compare t-values from permutation function and above\n",
    "\n",
    "rulecount = 0\n",
    "for ruledim in ruledims:\n",
    "    if np.sum(df_stats[ruledim]['t']==fwe_Ts[:,rulecount])==360:\n",
    "        print 'Correct t-values match up'\n",
    "    else:\n",
    "        print 'Error! Likely a bug in the code'\n",
    "    rulecount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out significant ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average significant IE for logic : 0.0228556752082\n",
      "Average significant T-stats for logic : 5.24410223409\n",
      "Maximum significant p-value for logic : 0.05\n",
      "----\n",
      "Average nonsignificant IE for logic : 0.0108887207719\n",
      "Average nonsignificant T-stats for logic : 2.14023612418\n",
      "Minimum nonsignificant p-value for logic : 0.0507\n",
      "\n",
      "\n",
      "*****************\n",
      "Average significant IE for sensory : 0.0213431959576\n",
      "Average significant T-stats for sensory : 4.97298583963\n",
      "Maximum significant p-value for sensory : 0.0458\n",
      "----\n",
      "Average nonsignificant IE for sensory : 0.0123727448245\n",
      "Average nonsignificant T-stats for sensory : 2.28815520011\n",
      "Minimum nonsignificant p-value for sensory : 0.0506\n",
      "\n",
      "\n",
      "*****************\n",
      "Average significant IE for motor : 0.0637508552622\n",
      "Average significant T-stats for motor : 6.80359722659\n",
      "Maximum significant p-value for motor : 0.0483\n",
      "----\n",
      "Average nonsignificant IE for motor : 0.00742946704093\n",
      "Average nonsignificant T-stats for motor : 1.55104069333\n",
      "Minimum nonsignificant p-value for motor : 0.0535\n",
      "\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "fwe_Ps2 = (1.0000 - fwe_Ps) # One-tailed test on upper tail\n",
    "sig_mat = fwe_Ps2 < 0.0500 # One-sided t-test (Only interested in values greater than 95% interval)\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'FWE_corrected_pvals_allruledims.csv'\n",
    "np.savetxt(outdir + filename, fwe_Ps, delimiter=',')\n",
    "\n",
    "sig_t = np.zeros((nParcels,len(ruledims)))\n",
    "sig_effect = np.zeros((nParcels,len(ruledims)))\n",
    "rulecount = 0\n",
    "for ruledim in ruledims:\n",
    "    sig_t[:,rulecount] = np.multiply(fwe_Ts[:,rulecount],sig_mat[:,rulecount])\n",
    "    sig_effect[:,rulecount] = np.multiply(effectsize[ruledim],sig_mat[:,rulecount])\n",
    "    \n",
    "    # Read out statistics for manuscript\n",
    "    # Identify significant regions\n",
    "    sig_ind = sig_mat[:,rulecount] == True\n",
    "    nonsig_ind = sig_mat[:,rulecount] == False\n",
    "    \n",
    "    print 'Average significant IE for', ruledim, ':', np.mean(effectsize[ruledim][sig_ind])\n",
    "    print 'Average significant T-stats for', ruledim, ':', np.mean(fwe_Ts[:,rulecount][sig_ind])\n",
    "    print 'Maximum significant p-value for', ruledim, ':', np.max(fwe_Ps2[:,rulecount][sig_ind])\n",
    "    print '----'\n",
    "    print 'Average nonsignificant IE for', ruledim, ':', np.mean(effectsize[ruledim][nonsig_ind])\n",
    "    print 'Average nonsignificant T-stats for', ruledim, ':', np.mean(fwe_Ts[:,rulecount][nonsig_ind])\n",
    "    print 'Minimum nonsignificant p-value for', ruledim, ':', np.min(fwe_Ps2[:,rulecount][nonsig_ind])\n",
    "    print '\\n'\n",
    "    print '*****************'\n",
    "    rulecount += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map out FWE-corrected statistics/results to surface using workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_t_vertex = np.zeros((len(glasser2),len(ruledims)))\n",
    "effects_vertex = np.zeros((len(glasser2),len(ruledims)))\n",
    "effects_vertex_sig = np.zeros((len(glasser2),len(ruledims)))\n",
    "col = 0\n",
    "for cols in range(sig_t_vertex.shape[1]):\n",
    "    for roi in range(nParcels):\n",
    "        parcel_ind = np.where(glasser2==(roi+1))[0]\n",
    "        sig_t_vertex[parcel_ind,col] = sig_t[roi,col]\n",
    "        effects_vertex_sig[parcel_ind,col] = sig_effect[roi,col]\n",
    "    col += 1\n",
    "\n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'RegionwiseIE_FWERThresholded_Tstat.csv'\n",
    "np.savetxt(outdir + filename, sig_t_vertex,fmt='%s')\n",
    "wb_file = 'RegionwiseIE_FWERThresholded_Tstat.dscalar.nii'\n",
    "glasserfilename = '/projects/AnalysisTools/ParcelsGlasser2016/archive/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "# # Compute effect size baseline (information content)\n",
    "# outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "# filename = 'RegionwiseIE_InformationEstimate.csv'\n",
    "# np.savetxt(outdir + filename, effects_vertex,fmt='%s')\n",
    "# wb_file = 'RegionwiseIE_InformationEstimate.dscalar.nii'\n",
    "# wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "# os.system(wb_command)\n",
    "\n",
    "# Compute Thresholded effect size baseline (information content)\n",
    "outdir = '/projects2/ModalityControl2/data/resultsMaster/Manuscript5_BaselineRegionIE/'\n",
    "filename = 'RegionwiseIE_FWERThresholded_InformationEstimate.csv'\n",
    "np.savetxt(outdir + filename, effects_vertex_sig,fmt='%s')\n",
    "wb_file = 'RegionwiseIE_FWERThresholded_InformationEstimate.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + ' ' + glasserfilename + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
